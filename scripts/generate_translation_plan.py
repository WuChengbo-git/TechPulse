#!/usr/bin/env python3
"""
ç”Ÿæˆå›½é™…åŒ–è¿ç§»è®¡åˆ’
åˆ†ææ‰«æç»“æœ,ç”Ÿæˆè¯¦ç»†çš„ç¿»è¯‘æ¸…å•å’Œè¿ç§»è®¡åˆ’
"""

import json
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Set

# å…³é”®çš„ç¿»è¯‘åˆ†ç±»
CATEGORY_MAPPING = {
    # Pages
    'llmproviderspage': 'llmProviders',
    'trendspage': 'trends',
    'settingspage': 'settings',
    'taskmanagementpage': 'taskManagement',
    'systemconfig': 'systemConfig',
    'systemstatuspage': 'systemStatus',
    'arxivpage': 'arxiv',
    'huggingfacepage': 'huggingface',
    'zennpage': 'zenn',
    'githubpage': 'github',
    'dashboard': 'dashboard',
    'datasources': 'dataSources',
    'analytics': 'analytics',
    'chat': 'chat',

    # Components
    'interestsurvey': 'onboarding',
    'languageselector': 'common',
    'sidebar': 'nav',
    'smartsearch': 'search',
    'recommendationpanel': 'recommendation',
    'qualitybadge': 'common',
}

class TranslationPlanGenerator:
    def __init__(self, scan_report_path: str):
        with open(scan_report_path, 'r', encoding='utf-8') as f:
            self.scan_data = json.load(f)

        self.stats = self.scan_data['statistics']
        self.findings = self.scan_data['detailed_findings']

        # åˆ†ç±»çš„ç¿»è¯‘é¡¹
        self.categorized_translations: Dict[str, List[Dict]] = defaultdict(list)
        self.unique_texts: Set[str] = set()

    def categorize_translations(self):
        """å°†ç¿»è¯‘é¡¹æŒ‰æ¨¡å—åˆ†ç±»"""
        for filepath, items in self.findings.items():
            # ä»æ–‡ä»¶è·¯å¾„æå–æ¨¡å—å
            filename = Path(filepath).stem.lower()

            # ç¡®å®šåˆ†ç±»
            category = CATEGORY_MAPPING.get(filename, 'common')

            for item in items:
                text = item['text']
                if text not in self.unique_texts:
                    self.unique_texts.add(text)

                    # ç”Ÿæˆå»ºè®®çš„key
                    key = self._suggest_key(text, category, filename)

                    self.categorized_translations[category].append({
                        'key': key,
                        'zh_CN': text,
                        'source_file': filepath,
                        'line': item['line'],
                        'context': item['line_content']
                    })

    def _suggest_key(self, text: str, category: str, filename: str) -> str:
        """æ™ºèƒ½ç”Ÿæˆç¿»è¯‘key"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        import re

        # å¸¸è§çš„keyæ˜ å°„
        key_hints = {
            'æˆåŠŸ': 'success',
            'å¤±è´¥': 'failed',
            'é”™è¯¯': 'error',
            'è­¦å‘Š': 'warning',
            'ç¡®è®¤': 'confirm',
            'å–æ¶ˆ': 'cancel',
            'ä¿å­˜': 'save',
            'åˆ é™¤': 'delete',
            'ç¼–è¾‘': 'edit',
            'æ·»åŠ ': 'add',
            'åŠ è½½': 'loading',
            'æµ‹è¯•': 'test',
            'è¿æ¥': 'connection',
            'é…ç½®': 'config',
            'æ¨¡å‹': 'model',
            'æä¾›å•†': 'provider',
            'è¯·': 'please',
            'å·²': 'already',
        }

        # å°è¯•ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
        for zh, en in key_hints.items():
            if zh in text:
                # ç®€åŒ–keyç”Ÿæˆ
                simplified = text[:15].strip()
                return f"{en}Message"

        # é»˜è®¤ä½¿ç”¨æè¿°æ€§key
        # è®¡ç®—æ–‡æœ¬é•¿åº¦ç”Ÿæˆåˆé€‚çš„key
        if len(text) <= 5:
            key_name = 'short_' + str(hash(text))[-6:]
        elif 'æˆåŠŸ' in text or 'ï¼' in text:
            key_name = 'successMessage'
        elif 'å¤±è´¥' in text or 'é”™è¯¯' in text:
            key_name = 'errorMessage'
        elif 'è¯·' in text or 'ï¼Ÿ' in text:
            key_name = 'promptMessage'
        else:
            key_name = 'message_' + str(hash(text))[-6:]

        return key_name

    def generate_translation_structure(self) -> Dict:
        """ç”Ÿæˆæ–°çš„ç¿»è¯‘ç»“æ„"""
        result = {}

        for category, items in self.categorized_translations.items():
            result[category] = {}

            for item in items:
                key = item['key']
                # é¿å…keyé‡å¤
                counter = 1
                original_key = key
                while key in result[category]:
                    key = f"{original_key}{counter}"
                    counter += 1

                result[category][key] = {
                    'zh-CN': item['zh_CN'],
                    'en-US': '[TODO]',  # éœ€è¦äººå·¥ç¿»è¯‘
                    'ja-JP': '[TODO]',  # éœ€è¦äººå·¥ç¿»è¯‘
                    '_source': {
                        'file': item['source_file'],
                        'line': item['line']
                    }
                }

        return result

    def generate_markdown_plan(self, output_file: str):
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¿ç§»è®¡åˆ’"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# å›½é™…åŒ–è¿ç§»è¯¦ç»†è®¡åˆ’\n\n")

            # æ€»è§ˆ
            f.write("## ğŸ“Š æ€»ä½“æƒ…å†µ\n\n")
            f.write(f"- **éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬æ€»æ•°**: {len(self.unique_texts)}\n")
            f.write(f"- **æ¶‰åŠæ–‡ä»¶æ•°**: {len(self.findings)}\n")
            f.write(f"- **éœ€è¦æ–°å¢çš„ç¿»è¯‘æ¨¡å—**: {len(self.categorized_translations)}\n\n")

            # æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ–‡ä»¶åˆ—è¡¨
            f.write("## ğŸ¯ è¿ç§»ä¼˜å…ˆçº§\n\n")
            f.write("æ ¹æ®å½±å“èŒƒå›´å’Œä½¿ç”¨é¢‘ç‡,å»ºè®®æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§è¿›è¡Œè¿ç§»:\n\n")

            # ç»Ÿè®¡å„æ–‡ä»¶çš„ç¡¬ç¼–ç æ•°é‡
            file_counts = [(k, len(v)) for k, v in self.findings.items()]
            file_counts.sort(key=lambda x: x[1], reverse=True)

            f.write("### ç¬¬ä¸€ä¼˜å…ˆçº§ (P0 - æ ¸å¿ƒé¡µé¢)\n\n")
            f.write("è¿™äº›æ˜¯ç”¨æˆ·æœ€å¸¸è®¿é—®çš„é¡µé¢,åº”æœ€å…ˆå®Œæˆ:\n\n")

            priority_files = [
                'LLMProvidersPage.tsx',
                'SettingsPage.tsx',
                'Dashboard.tsx',
                'InterestSurvey.tsx'
            ]

            for filepath, count in file_counts[:15]:
                filename = Path(filepath).name
                if any(pf in filename for pf in priority_files):
                    f.write(f"- [ ] `{filepath}` ({count} å¤„ç¡¬ç¼–ç )\n")

            f.write("\n### ç¬¬äºŒä¼˜å…ˆçº§ (P1 - åŠŸèƒ½é¡µé¢)\n\n")
            for filepath, count in file_counts[:15]:
                filename = Path(filepath).name
                if not any(pf in filename for pf in priority_files) and 'pages' in filepath:
                    f.write(f"- [ ] `{filepath}` ({count} å¤„ç¡¬ç¼–ç )\n")

            f.write("\n### ç¬¬ä¸‰ä¼˜å…ˆçº§ (P2 - ç»„ä»¶å’Œå·¥å…·)\n\n")
            for filepath, count in file_counts[:15]:
                if 'components' in filepath or 'utils' in filepath:
                    f.write(f"- [ ] `{filepath}` ({count} å¤„ç¡¬ç¼–ç )\n")

            # è¯¦ç»†çš„ç¿»è¯‘æ¸…å•
            f.write("\n## ğŸ“ è¯¦ç»†ç¿»è¯‘æ¸…å•\n\n")
            f.write("æŒ‰æ¨¡å—åˆ†ç±»çš„æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬:\n\n")

            for category in sorted(self.categorized_translations.keys()):
                items = self.categorized_translations[category]
                f.write(f"### {category} ({len(items)} é¡¹)\n\n")

                # å»é‡å¹¶æ’åº
                unique_items = {}
                for item in items:
                    text = item['zh_CN']
                    if text not in unique_items:
                        unique_items[text] = item

                f.write("| ä¸­æ–‡ | å»ºè®®Key | æ¥æºæ–‡ä»¶ |\n")
                f.write("|------|---------|----------|\n")

                for text, item in list(unique_items.items())[:50]:  # åªæ˜¾ç¤ºå‰50ä¸ª
                    key = item['key']
                    source = Path(item['source_file']).name
                    f.write(f"| {text[:30]} | `{category}.{key}` | {source} |\n")

                if len(unique_items) > 50:
                    f.write(f"\n_...è¿˜æœ‰ {len(unique_items) - 50} é¡¹_\n")

                f.write("\n")

            # è¿ç§»æ­¥éª¤æŒ‡å—
            f.write("## ğŸ› ï¸ è¿ç§»æ­¥éª¤\n\n")
            f.write("### æ­¥éª¤1: æ‰©å±• translations.ts\n\n")
            f.write("ä¸ºæ¯ä¸ªæ¨¡å—æ·»åŠ æ–°çš„ç¿»è¯‘keyã€‚å‚è€ƒç”Ÿæˆçš„ `translation_additions.json`\n\n")

            f.write("### æ­¥éª¤2: ä¿®æ”¹æºæ–‡ä»¶\n\n")
            f.write("å¯¹äºæ¯ä¸ªæ–‡ä»¶:\n\n")
            f.write("1. ç¡®è®¤æ–‡ä»¶å·²å¯¼å…¥ `useLanguage` hook\n")
            f.write("2. ä½¿ç”¨ `t('category.key')` æ›¿æ¢ç¡¬ç¼–ç æ–‡æœ¬\n")
            f.write("3. æµ‹è¯•ä¸­è‹±æ—¥ä¸‰ç§è¯­è¨€çš„æ˜¾ç¤º\n\n")

            f.write("### æ­¥éª¤3: éªŒè¯å’Œæµ‹è¯•\n\n")
            f.write("1. è¿è¡Œåº”ç”¨,åˆ‡æ¢è¯­è¨€éªŒè¯\n")
            f.write("2. æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„ç¡¬ç¼–ç \n")
            f.write("3. ç¡®ä¿æ‰€æœ‰UIå…ƒç´ éƒ½èƒ½æ­£ç¡®åˆ‡æ¢è¯­è¨€\n\n")

            # ç¤ºä¾‹ä»£ç 
            f.write("## ğŸ’¡ ä»£ç ä¿®æ”¹ç¤ºä¾‹\n\n")
            f.write("### ä¿®æ”¹å‰\n\n")
            f.write("```typescript\n")
            f.write("message.success('ä¿å­˜æˆåŠŸï¼');\n")
            f.write("const title = 'ç³»ç»Ÿè®¾ç½®';\n")
            f.write("```\n\n")

            f.write("### ä¿®æ”¹å\n\n")
            f.write("```typescript\n")
            f.write("import { useLanguage } from '../contexts/LanguageContext';\n\n")
            f.write("const { t } = useLanguage();\n")
            f.write("message.success(t('settings.saveSuccess'));\n")
            f.write("const title = t('settings.title');\n")
            f.write("```\n\n")

            # ä¼°ç®—å·¥ä½œé‡
            f.write("## â±ï¸ å·¥ä½œé‡ä¼°ç®—\n\n")
            total_items = len(self.unique_texts)
            f.write(f"- **ç¿»è¯‘å·¥ä½œ**: çº¦ {total_items} ä¸ªæ–‡æœ¬ Ã— 2è¯­è¨€ = {total_items * 2} æ¡ç¿»è¯‘\n")
            f.write(f"- **ä»£ç ä¿®æ”¹**: çº¦ {len(self.findings)} ä¸ªæ–‡ä»¶éœ€è¦ä¿®æ”¹\n")
            f.write(f"- **é¢„è®¡æ—¶é—´**: \n")
            f.write(f"  - ç¿»è¯‘: çº¦ {total_items * 2 // 60} å°æ—¶ (å‡è®¾æ¯åˆ†é’Ÿç¿»è¯‘1æ¡)\n")
            f.write(f"  - ä»£ç ä¿®æ”¹: çº¦ {len(self.findings) * 0.5:.1f} å°æ—¶ (å‡è®¾æ¯æ–‡ä»¶30åˆ†é’Ÿ)\n")
            f.write(f"  - æµ‹è¯•éªŒè¯: çº¦ 2-3 å°æ—¶\n")
            f.write(f"  - **æ€»è®¡**: çº¦ {total_items * 2 // 60 + len(self.findings) * 0.5 + 2.5:.1f} å°æ—¶\n\n")

    def generate_json_additions(self, output_file: str):
        """ç”Ÿæˆå¯ç›´æ¥æ·»åŠ åˆ°translations.tsçš„JSONæ ¼å¼"""
        structure = self.generate_translation_structure()

        # é‡æ–°ç»„ç»‡ä¸ºæ›´å‹å¥½çš„æ ¼å¼
        additions = {
            'zh-CN': {},
            'en-US': {},
            'ja-JP': {}
        }

        for category, items in structure.items():
            additions['zh-CN'][category] = {}
            additions['en-US'][category] = {}
            additions['ja-JP'][category] = {}

            for key, langs in items.items():
                additions['zh-CN'][category][key] = langs['zh-CN']
                additions['en-US'][category][key] = langs['en-US']
                additions['ja-JP'][category][key] = langs['ja-JP']

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(additions, f, ensure_ascii=False, indent=2)


def main():
    report_path = "/home/AI/TechPulse/reports/i18n_gaps_detailed.json"
    output_dir = Path("/home/AI/TechPulse/reports")

    print("ğŸ”„ æ­£åœ¨åˆ†ææ‰«æç»“æœ...")

    generator = TranslationPlanGenerator(report_path)
    generator.categorize_translations()

    print(f"âœ… åˆ†æå®Œæˆ:")
    print(f"   - ç‹¬ç‰¹æ–‡æœ¬æ•°: {len(generator.unique_texts)}")
    print(f"   - ç¿»è¯‘åˆ†ç±»æ•°: {len(generator.categorized_translations)}")

    print("\nğŸ“ æ­£åœ¨ç”Ÿæˆè¿ç§»è®¡åˆ’...")
    generator.generate_markdown_plan(str(output_dir / "i18n_migration_plan.md"))

    print("ğŸ“¦ æ­£åœ¨ç”Ÿæˆç¿»è¯‘è¡¥å……æ–‡ä»¶...")
    generator.generate_json_additions(str(output_dir / "translation_additions.json"))

    print(f"\n{'='*60}")
    print("âœ¨ è¿ç§»è®¡åˆ’ç”Ÿæˆå®Œæˆ!")
    print(f"\næŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶:")
    print(f"  ğŸ“„ è¿ç§»è®¡åˆ’: {output_dir / 'i18n_migration_plan.md'}")
    print(f"  ğŸ“¦ ç¿»è¯‘è¡¥å……: {output_dir / 'translation_additions.json'}")


if __name__ == "__main__":
    main()
