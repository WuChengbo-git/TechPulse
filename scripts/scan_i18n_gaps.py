#!/usr/bin/env python3
"""
å›½é™…åŒ–ç¼ºå¤±æ‰«æå·¥å…·
æ‰«æå‰ç«¯æºä»£ç ä¸­çš„ç¡¬ç¼–ç æ–‡æœ¬,ç”Ÿæˆéœ€è¦ç¿»è¯‘çš„æ¸…å•
"""

import re
import os
import json
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Tuple

# åŒ¹é…ä¸­æ–‡å­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
CHINESE_PATTERN = re.compile(r'[\u4e00-\u9fa5]+')
# åŒ¹é…æ—¥æ–‡å­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
JAPANESE_PATTERN = re.compile(r'[\u3040-\u309f\u30a0-\u30ff\u31f0-\u31ff]+')

# åŒ¹é…å­—ç¬¦ä¸²å­—é¢é‡ (å•å¼•å·ã€åŒå¼•å·ã€æ¨¡æ¿å­—ç¬¦ä¸²)
STRING_LITERAL_PATTERN = re.compile(
    r'''(?:['"`])((?:[^'"`\\]|\\.)*)(?:['"`])''',
    re.MULTILINE | re.DOTALL
)

# æ’é™¤çš„æ–‡ä»¶å’Œç›®å½•
EXCLUDE_PATTERNS = [
    'node_modules',
    'translations.ts',
    'translations-additions.ts',
    '.backup',
    'dist',
    'build'
]

# æ’é™¤çš„å¸¸è§æ¨¡å¼ (ä¸éœ€è¦ç¿»è¯‘çš„)
EXCLUDE_TEXT_PATTERNS = [
    r'^[\d\s\.\-\+\*/=<>!&|%()[\]{}:;,@#$^~`]+$',  # çº¯ç¬¦å·å’Œæ•°å­—
    r'^[a-zA-Z0-9\-_\.@]+$',  # çº¯è‹±æ–‡æ ‡è¯†ç¬¦
    r'^https?://',  # URL
    r'^/[a-zA-Z0-9\-_/]+$',  # è·¯ç”±è·¯å¾„
    r'^\w+\.\w+$',  # å±æ€§è®¿é—® like 'user.name'
    r'^(localStorage|sessionStorage|console|window|document)\.',  # JS API
    r'^(zh-CN|en-US|ja-JP)$',  # è¯­è¨€ä»£ç 
    r'^(GET|POST|PUT|DELETE|PATCH)$',  # HTTPæ–¹æ³•
    r'^\d{4}-\d{2}-\d{2}',  # æ—¥æœŸæ ¼å¼
    r'^[A-Z_]+$',  # å¸¸é‡å‘½å
    r'^#[0-9a-fA-F]{3,8}$',  # é¢œè‰²ä»£ç 
]

class I18nScanner:
    def __init__(self, src_dir: str):
        self.src_dir = Path(src_dir)
        self.findings: Dict[str, List[Dict]] = defaultdict(list)
        self.stats = {
            'total_files': 0,
            'files_with_issues': 0,
            'total_hardcoded': 0,
            'chinese_texts': 0,
            'japanese_texts': 0,
        }

    def should_exclude_file(self, filepath: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
        path_str = str(filepath)
        return any(pattern in path_str for pattern in EXCLUDE_PATTERNS)

    def should_exclude_text(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åº”è¯¥è¢«æ’é™¤ (ä¸éœ€è¦ç¿»è¯‘)"""
        text = text.strip()
        if not text or len(text) < 2:
            return True

        for pattern in EXCLUDE_TEXT_PATTERNS:
            if re.match(pattern, text):
                return True

        return False

    def extract_strings_from_file(self, filepath: Path) -> List[Tuple[int, str, str]]:
        """ä»æ–‡ä»¶ä¸­æå–æ‰€æœ‰å­—ç¬¦ä¸²å­—é¢é‡
        è¿”å›: [(è¡Œå·, åŸå§‹å†…å®¹, æå–çš„å­—ç¬¦ä¸²)]
        """
        results = []
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            for line_num, line in enumerate(lines, 1):
                # è·³è¿‡æ³¨é‡Šè¡Œ
                if line.strip().startswith('//') or line.strip().startswith('/*'):
                    continue

                # æŸ¥æ‰¾æ‰€æœ‰å­—ç¬¦ä¸²å­—é¢é‡
                for match in STRING_LITERAL_PATTERN.finditer(line):
                    text = match.group(1)
                    # å¤„ç†è½¬ä¹‰å­—ç¬¦
                    text = text.replace('\\n', ' ').replace('\\t', ' ')

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡æˆ–æ—¥æ–‡
                    if CHINESE_PATTERN.search(text) or JAPANESE_PATTERN.search(text):
                        if not self.should_exclude_text(text):
                            results.append((line_num, line.strip(), text))

        except Exception as e:
            print(f"âš ï¸  Error reading {filepath}: {e}")

        return results

    def scan_file(self, filepath: Path) -> None:
        """æ‰«æå•ä¸ªæ–‡ä»¶"""
        if self.should_exclude_file(filepath):
            return

        self.stats['total_files'] += 1
        strings = self.extract_strings_from_file(filepath)

        if strings:
            self.stats['files_with_issues'] += 1
            relative_path = filepath.relative_to(self.src_dir)

            for line_num, line_content, text in strings:
                # åˆ¤æ–­æ–‡æœ¬ç±»å‹
                has_chinese = bool(CHINESE_PATTERN.search(text))
                has_japanese = bool(JAPANESE_PATTERN.search(text))

                if has_chinese:
                    self.stats['chinese_texts'] += 1
                if has_japanese:
                    self.stats['japanese_texts'] += 1

                self.stats['total_hardcoded'] += 1

                self.findings[str(relative_path)].append({
                    'line': line_num,
                    'text': text,
                    'line_content': line_content,
                    'type': 'chinese' if has_chinese else 'japanese',
                    'length': len(text)
                })

    def scan_directory(self) -> None:
        """æ‰«ææ•´ä¸ªç›®å½•"""
        print(f"ğŸ” å¼€å§‹æ‰«æç›®å½•: {self.src_dir}")
        print(f"{'='*60}")

        # é€’å½’æ‰«ææ‰€æœ‰ .tsx å’Œ .ts æ–‡ä»¶
        for ext in ['*.tsx', '*.ts']:
            for filepath in self.src_dir.rglob(ext):
                if filepath.name.endswith('.d.ts'):
                    continue
                self.scan_file(filepath)

        print(f"âœ… æ‰«æå®Œæˆ!")
        print(f"   - æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"   - åŒ…å«ç¡¬ç¼–ç çš„æ–‡ä»¶: {self.stats['files_with_issues']}")
        print(f"   - ç¡¬ç¼–ç æ–‡æœ¬æ€»æ•°: {self.stats['total_hardcoded']}")
        print(f"   - ä¸­æ–‡æ–‡æœ¬: {self.stats['chinese_texts']}")
        print(f"   - æ—¥æ–‡æ–‡æœ¬: {self.stats['japanese_texts']}")

    def categorize_by_module(self) -> Dict[str, List]:
        """æŒ‰æ¨¡å—åˆ†ç±»å½’çº³"""
        categorized = {
            'pages': [],
            'components': [],
            'services': [],
            'utils': [],
            'others': []
        }

        for filepath, items in self.findings.items():
            if 'pages/' in filepath:
                category = 'pages'
            elif 'components/' in filepath:
                category = 'components'
            elif 'services/' in filepath:
                category = 'services'
            elif 'utils/' in filepath:
                category = 'utils'
            else:
                category = 'others'

            categorized[category].append({
                'file': filepath,
                'count': len(items),
                'items': items
            })

        return categorized

    def generate_report(self, output_file: str) -> None:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        categorized = self.categorize_by_module()

        report = {
            'scan_time': None,  # å¯ä»¥æ·»åŠ æ—¶é—´æˆ³
            'statistics': self.stats,
            'findings_by_module': categorized,
            'detailed_findings': dict(self.findings)
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

    def generate_markdown_report(self, output_file: str) -> None:
        """ç”ŸæˆMarkdownæ ¼å¼çš„å¯è¯»æŠ¥å‘Š"""
        categorized = self.categorize_by_module()

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# å›½é™…åŒ–ç¼ºå¤±æ‰«ææŠ¥å‘Š\n\n")

            # ç»Ÿè®¡æ‘˜è¦
            f.write("## ğŸ“Š æ‰«æç»Ÿè®¡\n\n")
            f.write(f"- **æ‰«ææ–‡ä»¶æ€»æ•°**: {self.stats['total_files']}\n")
            f.write(f"- **åŒ…å«ç¡¬ç¼–ç çš„æ–‡ä»¶**: {self.stats['files_with_issues']}\n")
            f.write(f"- **ç¡¬ç¼–ç æ–‡æœ¬æ€»æ•°**: {self.stats['total_hardcoded']}\n")
            f.write(f"- **ä¸­æ–‡æ–‡æœ¬**: {self.stats['chinese_texts']}\n")
            f.write(f"- **æ—¥æ–‡æ–‡æœ¬**: {self.stats['japanese_texts']}\n\n")

            # æŒ‰æ¨¡å—åˆ†ç±»
            f.write("## ğŸ“‚ æŒ‰æ¨¡å—åˆ†ç±»\n\n")

            for category, files in categorized.items():
                if not files:
                    continue

                total_in_category = sum(item['count'] for item in files)
                f.write(f"### {category.upper()} ({len(files)} æ–‡ä»¶, {total_in_category} å¤„ç¡¬ç¼–ç )\n\n")

                # æŒ‰ç¡¬ç¼–ç æ•°é‡æ’åº
                files_sorted = sorted(files, key=lambda x: x['count'], reverse=True)

                for file_info in files_sorted:
                    f.write(f"#### `{file_info['file']}` ({file_info['count']} å¤„)\n\n")

                    # åˆ—å‡ºå‰10ä¸ªç¤ºä¾‹
                    for i, item in enumerate(file_info['items'][:10], 1):
                        f.write(f"{i}. **ç¬¬ {item['line']} è¡Œ**: `{item['text']}`\n")
                        f.write(f"   ```typescript\n")
                        f.write(f"   {item['line_content']}\n")
                        f.write(f"   ```\n\n")

                    if file_info['count'] > 10:
                        f.write(f"   _...è¿˜æœ‰ {file_info['count'] - 10} å¤„ç¡¬ç¼–ç _\n\n")

            # ä¼˜å…ˆçº§å»ºè®®
            f.write("## ğŸ¯ ä¿®å¤ä¼˜å…ˆçº§å»ºè®®\n\n")
            f.write("æ ¹æ®ä½¿ç”¨é¢‘ç‡å’Œå½±å“èŒƒå›´,å»ºè®®æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§ä¿®å¤:\n\n")

            # è·å–ç¡¬ç¼–ç æœ€å¤šçš„å‰10ä¸ªæ–‡ä»¶
            all_files = []
            for category, files in categorized.items():
                all_files.extend(files)

            top_files = sorted(all_files, key=lambda x: x['count'], reverse=True)[:10]

            f.write("### é«˜ä¼˜å…ˆçº§ (ç¡¬ç¼–ç æ•°é‡æœ€å¤šçš„æ–‡ä»¶)\n\n")
            for i, file_info in enumerate(top_files, 1):
                f.write(f"{i}. `{file_info['file']}` - {file_info['count']} å¤„ç¡¬ç¼–ç \n")

            f.write("\n### å»ºè®®ä¿®å¤é¡ºåº\n\n")
            f.write("1. **Pages** - ç”¨æˆ·ç›´æ¥çœ‹åˆ°çš„é¡µé¢,å½±å“æœ€å¤§\n")
            f.write("2. **Components** - å¤ç”¨ç»„ä»¶,ä¿®æ”¹ä¸€æ¬¡å½±å“å¤šå¤„\n")
            f.write("3. **Services** - é”™è¯¯æ¶ˆæ¯å’Œæç¤º\n")
            f.write("4. **Utils** - å·¥å…·å‡½æ•°ä¸­çš„æ–‡æœ¬\n")

        print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

    def suggest_translation_keys(self) -> Dict[str, Dict]:
        """ä¸ºæ¯ä¸ªç¡¬ç¼–ç æ–‡æœ¬å»ºè®®ç¿»è¯‘key"""
        suggestions = {}

        for filepath, items in self.findings.items():
            # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­æ¨¡å—å
            parts = Path(filepath).parts
            if 'pages' in parts:
                module = Path(filepath).stem.replace('Page', '').lower()
            elif 'components' in parts:
                module = Path(filepath).stem.lower()
            else:
                module = 'common'

            for item in items:
                text = item['text']
                # ç”Ÿæˆå»ºè®®çš„key (ç®€åŒ–ç‰ˆ)
                # å®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦äººå·¥è°ƒæ•´
                key_suggestion = f"{module}.{self._generate_key_name(text)}"

                if text not in suggestions:
                    suggestions[text] = {
                        'suggested_key': key_suggestion,
                        'occurrences': [],
                        'chinese_text': text
                    }

                suggestions[text]['occurrences'].append({
                    'file': filepath,
                    'line': item['line']
                })

        return suggestions

    def _generate_key_name(self, text: str) -> str:
        """ä»ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆå»ºè®®çš„keyåç§°"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬,å®é™…å¯èƒ½éœ€è¦æ›´æ™ºèƒ½çš„å¤„ç†
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦,åªä¿ç•™ä¸­æ–‡å’Œå­—æ¯
        clean = re.sub(r'[^\w\u4e00-\u9fa5]', '', text)
        # æˆªæ–­åˆ°åˆç†é•¿åº¦
        if len(clean) > 20:
            clean = clean[:20]
        return clean.lower()


def main():
    # è®¾ç½®è·¯å¾„
    frontend_src = Path("/home/AI/TechPulse/frontend/src")
    output_dir = Path("/home/AI/TechPulse/reports")
    output_dir.mkdir(exist_ok=True)

    # åˆ›å»ºæ‰«æå™¨å¹¶æ‰§è¡Œæ‰«æ
    scanner = I18nScanner(frontend_src)
    scanner.scan_directory()

    # ç”ŸæˆæŠ¥å‘Š
    print(f"\n{'='*60}")
    print("ğŸ“ æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")

    scanner.generate_report(str(output_dir / "i18n_gaps_detailed.json"))
    scanner.generate_markdown_report(str(output_dir / "i18n_gaps_report.md"))

    # ç”Ÿæˆç¿»è¯‘keyå»ºè®®
    suggestions = scanner.suggest_translation_keys()
    with open(output_dir / "translation_key_suggestions.json", 'w', encoding='utf-8') as f:
        json.dump(suggestions, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ ç¿»è¯‘keyå»ºè®®å·²ä¿å­˜åˆ°: {output_dir / 'translation_key_suggestions.json'}")

    print(f"\n{'='*60}")
    print("âœ¨ æ‰«æå®Œæˆ! è¯·æŸ¥çœ‹ reports/ ç›®å½•ä¸‹çš„æŠ¥å‘Šæ–‡ä»¶")


if __name__ == "__main__":
    main()
