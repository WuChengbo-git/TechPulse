import requests
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import logging
from bs4 import BeautifulSoup
from ..summarization_service import get_summarization_service

logger = logging.getLogger(__name__)


class ZennScraper:
    def __init__(self):
        self.base_url = "https://zenn.dev"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        self.summarization_service = get_summarization_service()
    
    def _get_article_content(self, article_url: str) -> tuple[Optional[str], Optional[str]]:
        """
        ä»æ–‡ç« é¡µé¢è·å–å®Œæ•´å†…å®¹å’Œæ‘˜è¦ï¼ˆä¿ç•™Markdownæ ¼å¼ï¼‰

        Returns:
            tuple: (summary, full_content) - æ‘˜è¦ï¼ˆå‰300å­—ç¬¦ï¼‰å’Œå®Œæ•´Markdownå†…å®¹
        """
        try:
            response = requests.get(article_url, headers=self.headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # æ‰¾åˆ°æ–‡ç« æ­£æ–‡
            content_div = soup.find('div', class_='znc') or soup.find('article')

            if content_div:
                # å°†HTMLè½¬æ¢ä¸ºMarkdowné£æ ¼çš„æ–‡æœ¬
                markdown_text = self._html_to_markdown(content_div)

                # ç”Ÿæˆæ‘˜è¦ï¼ˆå‰300å­—ç¬¦ï¼‰
                summary = markdown_text[:300] + '...' if len(markdown_text) > 300 else markdown_text

                return summary, markdown_text

            return None, None
        except Exception as e:
            logger.warning(f"Error fetching article content from {article_url}: {e}")
            return None, None

    def _html_to_markdown(self, element) -> str:
        """
        å°†HTMLå…ƒç´ è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„æ–‡æœ¬
        ä¿ç•™ä»£ç å—ã€æ ‡é¢˜ã€åˆ—è¡¨ç­‰æ ¼å¼
        """
        markdown_lines = []

        for child in element.children:
            if child.name is None:  # æ–‡æœ¬èŠ‚ç‚¹
                text = str(child).strip()
                if text:
                    markdown_lines.append(text)
            elif child.name == 'h1':
                markdown_lines.append(f"\n# {child.get_text(strip=True)}\n")
            elif child.name == 'h2':
                markdown_lines.append(f"\n## {child.get_text(strip=True)}\n")
            elif child.name == 'h3':
                markdown_lines.append(f"\n### {child.get_text(strip=True)}\n")
            elif child.name == 'h4':
                markdown_lines.append(f"\n#### {child.get_text(strip=True)}\n")
            elif child.name == 'pre':
                # ä»£ç å—
                code = child.get_text(strip=True)
                lang = ''
                # å°è¯•è·å–è¯­è¨€æ ‡è¯†
                code_tag = child.find('code')
                if code_tag and code_tag.get('class'):
                    classes = code_tag.get('class', [])
                    for cls in classes:
                        if cls.startswith('language-'):
                            lang = cls.replace('language-', '')
                            break
                markdown_lines.append(f"\n```{lang}\n{code}\n```\n")
            elif child.name == 'code' and child.parent.name != 'pre':
                # è¡Œå†…ä»£ç 
                markdown_lines.append(f"`{child.get_text(strip=True)}`")
            elif child.name in ['ul', 'ol']:
                # åˆ—è¡¨
                for li in child.find_all('li', recursive=False):
                    prefix = '- ' if child.name == 'ul' else '1. '
                    markdown_lines.append(f"{prefix}{li.get_text(strip=True)}")
                markdown_lines.append("")
            elif child.name == 'p':
                markdown_lines.append(f"\n{child.get_text(strip=True)}\n")
            elif child.name == 'a':
                text = child.get_text(strip=True)
                href = child.get('href', '')
                markdown_lines.append(f"[{text}]({href})")
            elif child.name == 'blockquote':
                quote_text = child.get_text(strip=True)
                markdown_lines.append(f"\n> {quote_text}\n")
            else:
                # å…¶ä»–å…ƒç´ é€’å½’å¤„ç†
                if hasattr(child, 'children'):
                    markdown_lines.append(self._html_to_markdown(child))
                else:
                    text = child.get_text(strip=True) if hasattr(child, 'get_text') else str(child).strip()
                    if text:
                        markdown_lines.append(text)

        return '\n'.join(markdown_lines)

    def _get_article_summary(self, article_url: str) -> Optional[str]:
        """å‘åå…¼å®¹çš„æ–¹æ³•ï¼Œåªè¿”å›æ‘˜è¦"""
        summary, _ = self._get_article_content(article_url)
        return summary

    async def _get_article_with_ai_summary(
        self,
        article_url: str
    ) -> tuple[Optional[str], Optional[str], Optional[str]]:
        """
        è·å–æ–‡ç« å¹¶ç”ŸæˆAIæ‘˜è¦

        Returns:
            tuple: (short_summary, summary, full_content)
            - short_summary: 30å­—AIæ€»ç»“ï¼ˆåˆ—è¡¨å±•ç¤ºï¼‰
            - summary: 200å­—AIæ€»ç»“ï¼ˆå¿«é€Ÿé˜…è§ˆï¼‰
            - full_content: å®Œæ•´åŸæ–‡ï¼ˆæ·±åº¦é˜…è¯»ï¼‰
        """
        try:
            # è·å–å®Œæ•´æ–‡ç« å†…å®¹
            _, full_content = self._get_article_content(article_url)

            if not full_content:
                return None, None, None

            # ä½¿ç”¨AIç”Ÿæˆä¸åŒé•¿åº¦çš„æ‘˜è¦ï¼ˆZennæ˜¯æ—¥è¯­ç½‘ç«™ï¼Œç”Ÿæˆæ—¥è¯­æ‘˜è¦ï¼‰
            summaries = await self.summarization_service.generate_multi_length_summaries(
                full_content,
                language="ja"  # Zennæ–‡ç« ç”Ÿæˆæ—¥è¯­æ‘˜è¦
            )

            return summaries["short"], summaries["medium"], summaries["full"]

        except Exception as e:
            logger.error(f"ç”ŸæˆAIæ‘˜è¦å¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨æˆªæ–­çš„æ–‡æœ¬
            _, full_content = self._get_article_content(article_url)
            if full_content:
                short = full_content[:30] + "..." if len(full_content) > 30 else full_content
                medium = full_content[:200] + "..." if len(full_content) > 200 else full_content
                return short, medium, full_content
            return None, None, None

    async def get_trending_articles(self, limit: int = 20, min_likes: int = 20) -> List[Dict]:
        """
        è·å– Zenn çƒ­é—¨æŠ€æœ¯æ–‡ç«  - ä½¿ç”¨å®˜æ–¹APIå¹¶è·å–æ–‡ç« æ‘˜è¦
        """
        try:
            articles = []

            # ä½¿ç”¨ Zenn API
            api_url = f"{self.base_url}/api/articles"
            response = requests.get(api_url, headers=self.headers, timeout=30)
            response.raise_for_status()

            data = response.json()
            article_list = data.get('articles', [])

            for article in article_list:
                try:
                    likes = article.get('liked_count', 0)

                    # åº”ç”¨æœ€å°ã„ã„ã­ç­›é€‰
                    if likes < min_likes:
                        continue

                    article_url = f"{self.base_url}{article.get('path', '')}"

                    # è·å–æ–‡ç« å¹¶ç”ŸæˆAIæ‘˜è¦
                    short_summary, medium_summary, full_content = await self._get_article_with_ai_summary(article_url)

                    # æå–æ–‡ç« ä¿¡æ¯
                    article_data = {
                        'title': article.get('title', 'No Title'),
                        'url': article_url,
                        'short_summary': short_summary or f"{article.get('emoji', 'ğŸ“')} {article.get('title', 'No Title')[:30]}",  # 30å­—AIæ€»ç»“
                        'summary': medium_summary or short_summary,  # 200å­—AIæ€»ç»“
                        'content': full_content,  # å®Œæ•´åŸæ–‡
                        'author': article.get('user', {}).get('name', 'Unknown'),
                        'author_name': article.get('user', {}).get('username', ''),
                        'likes': likes,
                        'comments': article.get('comments_count', 0),
                        'emoji': article.get('emoji', 'ğŸ“'),
                        'published_at': article.get('published_at', ''),
                        'type': 'article',
                        'is_premium': False  # Zenn æ–‡ç« é»˜è®¤å…è´¹
                    }

                    articles.append(article_data)

                    # å¦‚æœå·²æ”¶é›†è¶³å¤Ÿæ•°é‡ï¼Œåœæ­¢
                    if len(articles) >= limit:
                        break
                except Exception as e:
                    logger.warning(f"Error parsing article: {e}")
                    continue

            logger.info(f"Fetched {len(articles)} articles from Zenn API")
            return articles
        except Exception as e:
            logger.error(f"Error fetching Zenn articles: {e}")
            return []

    async def get_tech_articles(self, limit: int = 20, min_likes: int = 20) -> List[Dict]:
        """
        è·å–æŠ€æœ¯ç›¸å…³æ–‡ç«  - ä½¿ç”¨APIå¹¶ç­›é€‰ç‚¹èµæ•°é«˜çš„æ–‡ç« 
        """
        try:
            articles = []

            # ä½¿ç”¨ Zenn API
            api_url = f"{self.base_url}/api/articles"
            response = requests.get(api_url, headers=self.headers, timeout=30)
            response.raise_for_status()

            data = response.json()
            article_list = data.get('articles', [])

            for article in article_list:
                try:
                    # åªä¿ç•™æŠ€æœ¯ç›¸å…³æ–‡ç« ï¼ˆæ ¹æ®ç‚¹èµæ•°ç­›é€‰ï¼‰
                    if article.get('liked_count', 0) >= min_likes:
                        article_url = f"{self.base_url}{article.get('path', '')}"

                        # è·å–æ–‡ç« å¹¶ç”ŸæˆAIæ‘˜è¦
                        short_summary, medium_summary, full_content = await self._get_article_with_ai_summary(article_url)

                        article_data = {
                            'title': article.get('title', 'No Title'),
                            'url': article_url,
                            'short_summary': short_summary or f"{article.get('emoji', 'ğŸ“')} {article.get('title', 'No Title')[:30]}",  # 30å­—AIæ€»ç»“
                            'summary': medium_summary or short_summary,  # 200å­—AIæ€»ç»“
                            'content': full_content,  # å®Œæ•´åŸæ–‡
                            'author': article.get('user', {}).get('name', 'Unknown'),
                            'author_name': article.get('user', {}).get('username', ''),
                            'likes': article.get('liked_count', 0),
                            'comments': article.get('comments_count', 0),
                            'emoji': article.get('emoji', 'ğŸ“'),
                            'published_at': article.get('published_at', ''),
                            'type': 'article',
                            'is_premium': False
                        }
                        articles.append(article_data)

                        if len(articles) >= limit:
                            break
                except Exception as e:
                    logger.warning(f"Error parsing tech article: {e}")
                    continue

            logger.info(f"Fetched {len(articles)} tech articles from Zenn")
            return articles
        except Exception as e:
            logger.error(f"Error fetching tech articles: {e}")
            return []

    async def get_recent_articles(self, days: int = 30) -> List[Dict]:
        """
        è·å–æœ€è¿‘Nå¤©çš„æ–‡ç«  - ä½¿ç”¨API
        """
        try:
            # Zenn API è¿”å›çš„å°±æ˜¯æœ€æ–°çš„æ–‡ç« ï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨ trending articles
            articles = await self.get_trending_articles(limit=30)
            logger.info(f"Fetched {len(articles)} recent articles from Zenn (last {days} days)")
            return articles
        except Exception as e:
            logger.error(f"Error fetching recent articles: {e}")
            return []

    async def get_article_details(self, url: str) -> Optional[Dict]:
        """
        è·å–æ–‡ç« è¯¦ç»†å†…å®¹ - åŸºæœ¬ä¿¡æ¯ç‰ˆæœ¬
        """
        try:
            # Zenn API ä¸æä¾›å®Œæ•´æ–‡ç« å†…å®¹ï¼Œè¿”å›åŸºæœ¬ç»“æ„
            return {
                'url': url,
                'content': '',
                'tags': []
            }
        except Exception as e:
            logger.error(f"Error fetching article details: {e}")
            return None

    # ä¿ç•™åŸæœ‰æ–¹æ³•ä½œä¸ºå¤‡ç”¨
    async def get_trending_articles_html(self, limit: int = 20) -> List[Dict]:
        """
        è·å– Zenn çƒ­é—¨æŠ€æœ¯æ–‡ç«  - HTMLè§£æç‰ˆæœ¬ï¼ˆå¤‡ç”¨ï¼‰
        """
        try:
            articles = []

            # è·å–æ–‡ç« é¡µé¢
            url = f"{self.base_url}/articles"
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
            article_items = soup.find_all('article') or soup.find_all('div', class_=lambda x: x and 'article' in x.lower())

            for item in article_items[:limit]:
                try:
                    # æå–æ–‡ç« é“¾æ¥
                    link_elem = item.find('a', href=True)
                    if not link_elem:
                        continue

                    article_url = link_elem['href']
                    if not article_url.startswith('http'):
                        article_url = f"{self.base_url}{article_url}"

                    # æå–æ ‡é¢˜
                    title_elem = item.find(['h1', 'h2', 'h3']) or link_elem
                    title = title_elem.get_text(strip=True) if title_elem else "No Title"

                    # æå–ä½œè€…ä¿¡æ¯
                    author_elem = item.find('span', class_=lambda x: x and 'author' in x.lower()) or \
                                 item.find('div', class_=lambda x: x and 'user' in x.lower())
                    author = author_elem.get_text(strip=True) if author_elem else "Unknown"
                    
                    # æå–æ—¶é—´ä¿¡æ¯
                    time_elem = item.find('time') or item.find('span', class_=lambda x: x and 'date' in x.lower())
                    published_at = time_elem.get_text(strip=True) if time_elem else ""
                    
                    # æå–emoji
                    emoji_elem = item.find('span', class_=lambda x: x and 'emoji' in x.lower())
                    emoji = emoji_elem.get_text(strip=True) if emoji_elem else "ğŸ“"
                    
                    # æå–ç‚¹èµæ•°æˆ–å…¶ä»–äº’åŠ¨æ•°æ®
                    likes_elem = item.find('span', class_=lambda x: x and ('like' in x.lower() or 'heart' in x.lower()))
                    likes = 0
                    if likes_elem:
                        try:
                            likes_text = likes_elem.get_text(strip=True)
                            likes = int(''.join(filter(str.isdigit, likes_text)))
                        except:
                            likes = 0
                    
                    article_data = {
                        "title": title,
                        "url": article_url,
                        "author": author,
                        "published_at": published_at,
                        "emoji": emoji,
                        "likes": likes,
                        "platform": "Zenn",
                        "language": "ja",
                        "raw_data": {
                            "scraped_at": datetime.now().isoformat(),
                            "source_url": url
                        }
                    }
                    
                    articles.append(article_data)
                    
                except Exception as e:
                    logger.warning(f"Error parsing article item: {e}")
                    continue
            
            logger.info(f"Successfully scraped {len(articles)} articles from Zenn")
            return articles

        except Exception as e:
            logger.error(f"Error fetching Zenn articles: {e}")
            return []