import requests
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import logging
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class ZennScraper:
    def __init__(self):
        self.base_url = "https://zenn.dev"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
    
    async def get_trending_articles(self, limit: int = 20) -> List[Dict]:
        """
        è·å– Zenn çƒ­é—¨æŠ€æœ¯æ–‡ç« 
        """
        try:
            articles = []
            
            # è·å–æ–‡ç« é¡µé¢
            url = f"{self.base_url}/articles"
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
            article_items = soup.find_all('article') or soup.find_all('div', class_=lambda x: x and 'article' in x.lower())
            
            for item in article_items[:limit]:
                try:
                    # æå–æ–‡ç« é“¾æ¥
                    link_elem = item.find('a', href=True)
                    if not link_elem:
                        continue
                    
                    article_url = link_elem['href']
                    if not article_url.startswith('http'):
                        article_url = f"{self.base_url}{article_url}"
                    
                    # æå–æ ‡é¢˜
                    title_elem = item.find(['h1', 'h2', 'h3']) or link_elem
                    title = title_elem.get_text(strip=True) if title_elem else "No Title"
                    
                    # æå–ä½œè€…ä¿¡æ¯
                    author_elem = item.find('span', class_=lambda x: x and 'author' in x.lower()) or \
                                 item.find('div', class_=lambda x: x and 'user' in x.lower())
                    author = author_elem.get_text(strip=True) if author_elem else "Unknown"
                    
                    # æå–æ—¶é—´ä¿¡æ¯
                    time_elem = item.find('time') or item.find('span', class_=lambda x: x and 'date' in x.lower())
                    published_at = time_elem.get_text(strip=True) if time_elem else ""
                    
                    # æå–emoji
                    emoji_elem = item.find('span', class_=lambda x: x and 'emoji' in x.lower())
                    emoji = emoji_elem.get_text(strip=True) if emoji_elem else "ğŸ“"
                    
                    # æå–ç‚¹èµæ•°æˆ–å…¶ä»–äº’åŠ¨æ•°æ®
                    likes_elem = item.find('span', class_=lambda x: x and ('like' in x.lower() or 'heart' in x.lower()))
                    likes = 0
                    if likes_elem:
                        try:
                            likes_text = likes_elem.get_text(strip=True)
                            likes = int(''.join(filter(str.isdigit, likes_text)))
                        except:
                            likes = 0
                    
                    article_data = {
                        "title": title,
                        "url": article_url,
                        "author": author,
                        "published_at": published_at,
                        "emoji": emoji,
                        "likes": likes,
                        "platform": "Zenn",
                        "language": "ja",
                        "raw_data": {
                            "scraped_at": datetime.now().isoformat(),
                            "source_url": url
                        }
                    }
                    
                    articles.append(article_data)
                    
                except Exception as e:
                    logger.warning(f"Error parsing article item: {e}")
                    continue
            
            logger.info(f"Successfully scraped {len(articles)} articles from Zenn")
            return articles
            
        except Exception as e:
            logger.error(f"Error fetching Zenn articles: {e}")
            return []
    
    async def get_tech_articles(self, limit: int = 20) -> List[Dict]:
        """
        è·å–æŠ€æœ¯ç±»æ–‡ç« ï¼Œé€šè¿‡æœç´¢å…³é”®è¯
        """
        try:
            all_articles = []
            
            # æŠ€æœ¯å…³é”®è¯
            tech_keywords = [
                "AI", "æ©Ÿæ¢°å­¦ç¿’", "Python", "JavaScript", "React", "Vue",
                "Docker", "Kubernetes", "AWS", "ã‚¯ãƒ©ã‚¦ãƒ‰", "DevOps",
                "ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰", "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "API"
            ]
            
            for keyword in tech_keywords[:3]:  # é™åˆ¶æœç´¢æ¬¡æ•°
                try:
                    # ä½¿ç”¨æœç´¢åŠŸèƒ½
                    search_url = f"{self.base_url}/search"
                    params = {"q": keyword}
                    
                    response = requests.get(search_url, params=params, headers=self.headers, timeout=30)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æŸ¥æ‰¾æœç´¢ç»“æœ
                        article_items = soup.find_all('article') or soup.find_all('div', class_=lambda x: x and 'search-result' in x.lower())
                        
                        for item in article_items[:5]:  # æ¯ä¸ªå…³é”®è¯å–5ç¯‡
                            try:
                                link_elem = item.find('a', href=True)
                                if not link_elem:
                                    continue
                                
                                article_url = link_elem['href']
                                if not article_url.startswith('http'):
                                    article_url = f"{self.base_url}{article_url}"
                                
                                # é¿å…é‡å¤
                                if any(art["url"] == article_url for art in all_articles):
                                    continue
                                
                                title_elem = item.find(['h1', 'h2', 'h3']) or link_elem
                                title = title_elem.get_text(strip=True) if title_elem else "No Title"
                                
                                author_elem = item.find('span', class_=lambda x: x and 'author' in x.lower())
                                author = author_elem.get_text(strip=True) if author_elem else "Unknown"
                                
                                article_data = {
                                    "title": title,
                                    "url": article_url,
                                    "author": author,
                                    "keyword": keyword,
                                    "platform": "Zenn",
                                    "language": "ja",
                                    "raw_data": {
                                        "scraped_at": datetime.now().isoformat(),
                                        "search_keyword": keyword
                                    }
                                }
                                
                                all_articles.append(article_data)
                                
                            except Exception as e:
                                logger.warning(f"Error parsing search result: {e}")
                                continue
                                
                except Exception as e:
                    logger.warning(f"Error searching for keyword {keyword}: {e}")
                    continue
            
            # æŒ‰æ ‡é¢˜å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_articles = list({art["title"]: art for art in all_articles}.values())
            return unique_articles[:limit]
            
        except Exception as e:
            logger.error(f"Error fetching tech articles from Zenn: {e}")
            return []
    
    async def get_article_details(self, article_url: str) -> Optional[Dict]:
        """
        è·å–æ–‡ç« è¯¦ç»†å†…å®¹
        """
        try:
            response = requests.get(article_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ–‡ç« å†…å®¹
            title_elem = soup.find('h1')
            title = title_elem.get_text(strip=True) if title_elem else "No Title"
            
            # æå–æ­£æ–‡å†…å®¹çš„å‰500å­—ç¬¦
            content_elem = soup.find('article') or soup.find('div', class_=lambda x: x and 'content' in x.lower())
            content = ""
            if content_elem:
                # ç§»é™¤ä»£ç å—å’Œå…¶ä»–ä¸å¿…è¦çš„å…ƒç´ 
                for code in content_elem.find_all('code'):
                    code.decompose()
                for pre in content_elem.find_all('pre'):
                    pre.decompose()
                    
                content = content_elem.get_text(strip=True)[:500]
            
            # æå–æ ‡ç­¾
            tags = []
            tag_elements = soup.find_all('span', class_=lambda x: x and 'tag' in x.lower()) or \
                          soup.find_all('a', class_=lambda x: x and 'tag' in x.lower())
            
            for tag_elem in tag_elements:
                tag_text = tag_elem.get_text(strip=True)
                if tag_text and tag_text not in tags:
                    tags.append(tag_text)
            
            # æå–ä½œè€…ä¿¡æ¯
            author_elem = soup.find('span', class_=lambda x: x and 'author' in x.lower()) or \
                         soup.find('div', class_=lambda x: x and 'user' in x.lower())
            author = author_elem.get_text(strip=True) if author_elem else "Unknown"
            
            return {
                "title": title,
                "url": article_url,
                "author": author,
                "content": content,
                "tags": tags[:5],  # é™åˆ¶æ ‡ç­¾æ•°é‡
                "platform": "Zenn",
                "language": "ja",
                "scraped_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error fetching article details from {article_url}: {e}")
            return None
    
    async def get_recent_articles(self, days: int = 30) -> List[Dict]:
        """
        è·å–æœ€è¿‘å‡ å¤©çš„æ–‡ç«  - æ”¹è¿›ç‰ˆï¼Œè·å–ä¸€ä¸ªæœˆå†…çš„æ´»è·ƒæ–‡ç« 
        """
        try:
            all_articles = []
            
            # ç­–ç•¥1: è·å–çƒ­é—¨æ–‡ç« é¡µé¢
            trending_articles = await self.get_trending_articles(limit=30)
            all_articles.extend(trending_articles)
            
            # ç­–ç•¥2: é€šè¿‡æœ€æ–°æ–‡ç« APIè·å–
            try:
                latest_url = f"{self.base_url}/articles?order=latest"
                response = requests.get(latest_url, headers=self.headers, timeout=30)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    article_items = soup.find_all('article')[:30]
                    
                    for item in article_items:
                        try:
                            link_elem = item.find('a', href=True)
                            if not link_elem:
                                continue
                            
                            article_url = link_elem['href']
                            if not article_url.startswith('http'):
                                article_url = f"{self.base_url}{article_url}"
                            
                            # é¿å…é‡å¤
                            if any(art["url"] == article_url for art in all_articles):
                                continue
                            
                            title_elem = item.find(['h1', 'h2', 'h3']) or link_elem
                            title = title_elem.get_text(strip=True) if title_elem else "No Title"
                            
                            # æå–æ—¶é—´ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            time_elem = item.find('time')
                            published_at = ""
                            if time_elem:
                                published_at = time_elem.get('datetime') or time_elem.get_text(strip=True)
                            
                            article_data = {
                                "title": title,
                                "url": article_url,
                                "published_at": published_at,
                                "platform": "Zenn",
                                "language": "ja",
                                "type": "latest",
                                "raw_data": {
                                    "scraped_at": datetime.now().isoformat(),
                                    "source": "latest_page"
                                }
                            }
                            
                            all_articles.append(article_data)
                            
                        except Exception as e:
                            logger.warning(f"Error parsing latest article: {e}")
                            continue
                            
            except Exception as e:
                logger.warning(f"Error fetching latest articles: {e}")
            
            # ç­–ç•¥3: è·å–AI/æŠ€æœ¯ç›¸å…³çš„çƒ­é—¨æ–‡ç« 
            tech_articles = await self.get_tech_articles(limit=20)
            for article in tech_articles:
                if not any(art["url"] == article["url"] for art in all_articles):
                    all_articles.append(article)
            
            # å»é‡å¹¶æŒ‰æ ‡é¢˜æ’åº
            unique_articles = list({art["title"]: art for art in all_articles}.values())
            
            # å¦‚æœå¯èƒ½çš„è¯ï¼ŒæŒ‰æ—¶é—´è¿‡æ»¤ï¼ˆZennçš„æ—¶é—´ä¿¡æ¯æœ‰é™ï¼‰
            filtered_articles = []
            cutoff_date = datetime.now() - timedelta(days=days)
            
            for article in unique_articles:
                include_article = True
                
                # å¦‚æœæœ‰æ—¶é—´ä¿¡æ¯ï¼Œå°è¯•è§£æå¹¶è¿‡æ»¤
                if article.get("published_at"):
                    try:
                        # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
                        time_str = article["published_at"]
                        article_date = None
                        
                        # ISOæ ¼å¼
                        try:
                            article_date = datetime.fromisoformat(time_str.replace('Z', '+00:00').replace('+00:00', ''))
                        except:
                            pass
                        
                        # ç›¸å¯¹æ—¶é—´æ ¼å¼ï¼ˆå¦‚"2æ—¥å‰"ï¼‰
                        if not article_date and "æ—¥å‰" in time_str:
                            import re
                            match = re.search(r'(\d+)æ—¥å‰', time_str)
                            if match:
                                days_ago = int(match.group(1))
                                article_date = datetime.now() - timedelta(days=days_ago)
                        
                        # å¦‚æœæˆåŠŸè§£ææ—¶é—´ä¸”è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡
                        if article_date and article_date < cutoff_date:
                            include_article = False
                            
                    except Exception as e:
                        logger.debug(f"Error parsing date {article.get('published_at')}: {e}")
                        # è§£æå¤±è´¥æ—¶ä»ç„¶åŒ…å«æ–‡ç« 
                        pass
                
                if include_article:
                    filtered_articles.append(article)
            
            # é™åˆ¶è¿”å›æ•°é‡
            result = filtered_articles[:30]
            logger.info(f"Fetched {len(result)} recent Zenn articles from last {days} days")
            return result
            
        except Exception as e:
            logger.error(f"Error fetching recent Zenn articles: {e}")
            return []