from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, HttpUrl
from typing import Optional, List, Dict, Any
import requests
from bs4 import BeautifulSoup
import logging
from ..services.ai.azure_openai import azure_openai_service
from ..services.ai.config_helper import get_ai_service_for_user
from ..core.config import settings
from ..api.auth import get_current_user
from ..models.user import User
import re
import json

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chat", tags=["chat"])


class URLAnalysisRequest(BaseModel):
    url: HttpUrl
    custom_prompt: Optional[str] = None


class URLAnalysisResponse(BaseModel):
    url: str
    title: str
    content_summary: str
    key_points: List[str]
    analysis: str
    tags: List[str]
    content_type: str
    error: Optional[str] = None


class ChatMessage(BaseModel):
    message: str
    context_url: Optional[str] = None
    conversation_history: Optional[List[Dict[str, str]]] = []


class ChatResponse(BaseModel):
    response: str
    sources: Optional[List[str]] = None
    suggestions: Optional[List[str]] = None


class WebContentExtractor:
    """ç½‘é¡µå†…å®¹æå–å™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def extract_content(self, url: str) -> Dict[str, Any]:
        """æå–ç½‘é¡µå†…å®¹"""
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # æå–æ ‡é¢˜
            title = ""
            if soup.title:
                title = soup.title.string.strip()
            elif soup.find('h1'):
                title = soup.find('h1').get_text().strip()
            
            # æå–ä¸»è¦å†…å®¹
            content = ""
            
            # å°è¯•ä¸åŒçš„å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'article', 'main', '.content', '.post', '.entry',
                '[role="main"]', '.article-content', '.post-content'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(separator='\n', strip=True)
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œä½¿ç”¨body
            if not content:
                content = soup.body.get_text(separator='\n', strip=True) if soup.body else ""
            
            # æ¸…ç†å†…å®¹
            content = self._clean_content(content)
            
            # æ£€æµ‹å†…å®¹ç±»å‹
            content_type = self._detect_content_type(url, title, content)
            
            # æå–å…³é”®ä¿¡æ¯
            metadata = self._extract_metadata(soup)
            
            return {
                'title': title,
                'content': content[:5000],  # é™åˆ¶å†…å®¹é•¿åº¦
                'content_type': content_type,
                'metadata': metadata,
                'url': url
            }
            
        except Exception as e:
            logger.error(f"Error extracting content from {url}: {e}")
            raise HTTPException(status_code=400, detail=f"æ— æ³•æå–ç½‘é¡µå†…å®¹: {str(e)}")
    
    def _clean_content(self, content: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œ
        content = re.sub(r'\n\s*\n', '\n\n', content)
        # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼
        content = '\n'.join(line.strip() for line in content.split('\n'))
        # é™åˆ¶è¿ç»­ç©ºè¡Œ
        content = re.sub(r'\n{3,}', '\n\n', content)
        return content.strip()
    
    def _detect_content_type(self, url: str, title: str, content: str) -> str:
        """æ£€æµ‹å†…å®¹ç±»å‹"""
        url_lower = url.lower()
        title_lower = title.lower()
        content_lower = content.lower()
        
        # GitHubæ£€æµ‹
        if 'github.com' in url_lower:
            return 'github_repository'
        
        # æŠ€æœ¯åšå®¢æ£€æµ‹
        blog_indicators = ['blog', 'article', 'post', 'tutorial']
        if any(indicator in url_lower for indicator in blog_indicators):
            return 'tech_blog'
        
        # æ–‡æ¡£æ£€æµ‹
        doc_indicators = ['docs', 'documentation', 'manual', 'guide']
        if any(indicator in url_lower for indicator in doc_indicators):
            return 'documentation'
        
        # å­¦æœ¯è®ºæ–‡æ£€æµ‹
        if 'arxiv.org' in url_lower or 'paper' in title_lower:
            return 'academic_paper'
        
        # æ–°é—»æ£€æµ‹
        news_indicators = ['news', 'æ–°é—»', 'report', 'announcement']
        if any(indicator in title_lower for indicator in news_indicators):
            return 'news'
        
        # æŠ€æœ¯å†…å®¹æ£€æµ‹
        tech_keywords = ['api', 'code', 'programming', 'development', 'framework', 'library']
        if any(keyword in content_lower for keyword in tech_keywords):
            return 'technical_content'
        
        return 'general_web_page'
    
    def _extract_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:
        """æå–ç½‘é¡µå…ƒæ•°æ®"""
        metadata = {}
        
        # æå–description
        description_meta = soup.find('meta', attrs={'name': 'description'}) or \
                          soup.find('meta', attrs={'property': 'og:description'})
        if description_meta:
            metadata['description'] = description_meta.get('content', '')
        
        # æå–keywords
        keywords_meta = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_meta:
            metadata['keywords'] = keywords_meta.get('content', '')
        
        # æå–author
        author_meta = soup.find('meta', attrs={'name': 'author'})
        if author_meta:
            metadata['author'] = author_meta.get('content', '')
        
        return metadata


# å…¨å±€å®ä¾‹
web_extractor = WebContentExtractor()


@router.post("/analyze-url", response_model=URLAnalysisResponse)
async def analyze_url(
    request: URLAnalysisRequest,
    current_user: User = Depends(get_current_user)
):
    """
    åˆ†æç½‘é¡µURLï¼Œæå–å†…å®¹å¹¶è¿›è¡ŒAIåˆ†æ
    ä½¿ç”¨å½“å‰ç”¨æˆ·çš„AIé…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
    """
    try:
        # è·å–ç”¨æˆ·çš„AIæœåŠ¡å®ä¾‹
        user_ai_service = get_ai_service_for_user(current_user.id)

        # æå–ç½‘é¡µå†…å®¹
        extracted_data = web_extractor.extract_content(str(request.url))

        if not user_ai_service.is_available():
            raise HTTPException(status_code=503, detail="AIæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®æ‚¨çš„AIæ¨¡å‹")
        
        # å‡†å¤‡åˆ†æå†…å®¹
        analysis_content = f"""
        æ ‡é¢˜: {extracted_data['title']}
        ç½‘å€: {extracted_data['url']}
        å†…å®¹ç±»å‹: {extracted_data['content_type']}
        å†…å®¹: {extracted_data['content'][:3000]}
        """
        
        # è‡ªå®šä¹‰åˆ†ææç¤ºè¯
        if request.custom_prompt:
            analysis_prompt = f"{request.custom_prompt}\n\nåŸºäºä»¥ä¸‹ç½‘é¡µå†…å®¹è¿›è¡Œåˆ†æ:\n{analysis_content}"
        else:
            analysis_prompt = f"""
            è¯·å¯¹ä»¥ä¸‹ç½‘é¡µå†…å®¹è¿›è¡Œè¯¦ç»†åˆ†æï¼š
            
            {analysis_content}
            
            è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›åˆ†æï¼š
            
            ğŸ“ **å†…å®¹æ‘˜è¦** (50-100å­—ç®€æ´æ¦‚æ‹¬)
            
            ğŸ” **å…³é”®è¦ç‚¹** (3-5ä¸ªè¦ç‚¹ï¼Œæ¯ä¸ªè¦ç‚¹ä¸€è¡Œ)
            - è¦ç‚¹1
            - è¦ç‚¹2
            - è¦ç‚¹3
            
            ğŸ’¡ **æ·±åº¦åˆ†æ** (è¯¦ç»†åˆ†æå†…å®¹ä»·å€¼ã€æŠ€æœ¯æ·±åº¦ã€å®ç”¨æ€§ç­‰)
            
            ğŸ·ï¸ **ç›¸å…³æ ‡ç­¾** (5-8ä¸ªæ ‡ç­¾ï¼Œå¦‚ï¼šæŠ€æœ¯æ ˆã€åº”ç”¨åœºæ™¯ã€éš¾åº¦ç­‰)
            
            â­ **æ¨èæŒ‡æ•°** (1-5æ˜Ÿï¼Œè¯´æ˜æ¨èç†ç”±)
            """
        
        # è°ƒç”¨AIåˆ†æï¼ˆä½¿ç”¨ç”¨æˆ·çš„AIæœåŠ¡ï¼‰
        analysis_result = await user_ai_service.summarize_content(
            analysis_prompt,
            extracted_data['content_type'],
            settings.default_language
        )

        # æå–æ ‡ç­¾
        tags = await user_ai_service.extract_tags(
            analysis_content,
            settings.default_language
        )
        
        # è§£æå…³é”®è¦ç‚¹
        key_points = []
        if analysis_result:
            # å°è¯•ä»åˆ†æç»“æœä¸­æå–è¦ç‚¹
            lines = analysis_result.split('\n')
            in_key_points = False
            for line in lines:
                line = line.strip()
                if 'å…³é”®è¦ç‚¹' in line or 'è¦ç‚¹' in line:
                    in_key_points = True
                    continue
                elif line.startswith('-') and in_key_points:
                    key_points.append(line[1:].strip())
                elif line.startswith('ğŸ’¡') or line.startswith('ğŸ·ï¸'):
                    in_key_points = False
        
        # å¦‚æœæ²¡æœ‰è§£æåˆ°è¦ç‚¹ï¼Œç”Ÿæˆé»˜è®¤è¦ç‚¹
        if not key_points:
            key_points = [
                f"ç½‘é¡µç±»å‹: {extracted_data['content_type']}",
                f"å†…å®¹é•¿åº¦: {len(extracted_data['content'])} å­—ç¬¦",
                "éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
            ]
        
        return URLAnalysisResponse(
            url=str(request.url),
            title=extracted_data['title'],
            content_summary=analysis_result or "AIåˆ†ææš‚æ—¶ä¸å¯ç”¨",
            key_points=key_points[:5],  # é™åˆ¶è¦ç‚¹æ•°é‡
            analysis=analysis_result or "AIåˆ†ææš‚æ—¶ä¸å¯ç”¨",
            tags=tags or ['ç½‘é¡µå†…å®¹'],
            content_type=extracted_data['content_type']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analyzing URL {request.url}: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


@router.post("/chat", response_model=ChatResponse)
async def chat_with_ai(
    request: ChatMessage,
    current_user: User = Depends(get_current_user)
):
    """
    ä¸AIèŠå¤©ï¼Œæ”¯æŒåŸºäºç½‘é¡µå†…å®¹çš„é—®ç­”
    ä½¿ç”¨å½“å‰ç”¨æˆ·çš„AIé…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
    """
    try:
        # è·å–ç”¨æˆ·çš„AIæœåŠ¡å®ä¾‹
        user_ai_service = get_ai_service_for_user(current_user.id)

        if not user_ai_service.is_available():
            raise HTTPException(status_code=503, detail="AIæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®æ‚¨çš„AIæ¨¡å‹")
        
        # å‡†å¤‡å¯¹è¯ä¸Šä¸‹æ–‡
        context = ""
        if request.context_url:
            try:
                # å¦‚æœæœ‰ä¸Šä¸‹æ–‡URLï¼Œå…ˆæå–å†…å®¹
                extracted_data = web_extractor.extract_content(request.context_url)
                context = f"""
                å‚è€ƒç½‘é¡µ: {extracted_data['title']}
                ç½‘å€: {extracted_data['url']}
                å†…å®¹æ‘˜è¦: {extracted_data['content'][:1500]}
                """
            except Exception as e:
                logger.warning(f"Failed to extract context from URL {request.context_url}: {e}")
        
        # æ„å»ºå¯¹è¯å†å²
        conversation = ""
        if request.conversation_history:
            for msg in request.conversation_history[-5:]:  # åªä¿ç•™æœ€è¿‘5è½®å¯¹è¯
                role = msg.get('role', 'user')
                content = msg.get('content', '')
                conversation += f"{role}: {content}\n"
        
        # æ„å»ºå®Œæ•´çš„å¯¹è¯æç¤º
        chat_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æç½‘é¡µå†…å®¹å’Œå›ç­”æŠ€æœ¯é—®é¢˜ã€‚
        
        {context}
        
        å¯¹è¯å†å²:
        {conversation}
        
        ç”¨æˆ·é—®é¢˜: {request.message}
        
        è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ç»™å‡ºå‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚å¦‚æœé—®é¢˜ä¸æä¾›çš„ç½‘é¡µå†…å®¹ç›¸å…³ï¼Œè¯·ç»“åˆå†…å®¹è¿›è¡Œå›ç­”ã€‚
        """
        
        # è·å–AIå›å¤ï¼ˆä½¿ç”¨ç”¨æˆ·çš„AIæœåŠ¡ï¼‰
        ai_response = await user_ai_service.summarize_content(
            chat_prompt,
            "chat",
            settings.default_language
        )
        
        # ç”Ÿæˆå»ºè®®é—®é¢˜
        suggestions = []
        if request.context_url:
            suggestions = [
                "è¿™ä¸ªå†…å®¹çš„ä¸»è¦æŠ€æœ¯æ ˆæ˜¯ä»€ä¹ˆï¼Ÿ",
                "å¦‚ä½•å¿«é€Ÿä¸Šæ‰‹è¿™ä¸ªé¡¹ç›®ï¼Ÿ",
                "è¿™ä¸ªå†…å®¹çš„å®ç”¨ä»·å€¼å¦‚ä½•ï¼Ÿ",
                "æœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼Ÿ"
            ]
        else:
            suggestions = [
                "èƒ½è¯¦ç»†è§£é‡Šä¸€ä¸‹å—ï¼Ÿ",
                "æœ‰ä»€ä¹ˆç›¸å…³çš„æœ€ä½³å®è·µï¼Ÿ",
                "è¿™ä¸ªæŠ€æœ¯çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "å¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨ï¼Ÿ"
            ]
        
        return ChatResponse(
            response=ai_response or "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚",
            sources=[request.context_url] if request.context_url else None,
            suggestions=suggestions
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in chat: {e}")
        raise HTTPException(status_code=500, detail=f"èŠå¤©å¤±è´¥: {str(e)}")


@router.get("/content-types")
async def get_supported_content_types():
    """
    è·å–æ”¯æŒçš„å†…å®¹ç±»å‹
    """
    return {
        "content_types": [
            {
                "type": "github_repository",
                "name": "GitHubä»“åº“",
                "description": "ä»£ç ä»“åº“ã€å¼€æºé¡¹ç›®"
            },
            {
                "type": "tech_blog", 
                "name": "æŠ€æœ¯åšå®¢",
                "description": "æŠ€æœ¯æ–‡ç« ã€æ•™ç¨‹"
            },
            {
                "type": "documentation",
                "name": "æŠ€æœ¯æ–‡æ¡£",
                "description": "APIæ–‡æ¡£ã€ä½¿ç”¨æ‰‹å†Œ"
            },
            {
                "type": "academic_paper",
                "name": "å­¦æœ¯è®ºæ–‡",
                "description": "ç ”ç©¶è®ºæ–‡ã€arXivæ–‡ç« "
            },
            {
                "type": "news",
                "name": "æŠ€æœ¯æ–°é—»",
                "description": "è¡Œä¸šåŠ¨æ€ã€äº§å“å‘å¸ƒ"
            },
            {
                "type": "technical_content",
                "name": "æŠ€æœ¯å†…å®¹",
                "description": "æŠ€æœ¯ç›¸å…³çš„ä¸€èˆ¬å†…å®¹"
            },
            {
                "type": "general_web_page",
                "name": "æ™®é€šç½‘é¡µ",
                "description": "å…¶ä»–ç±»å‹çš„ç½‘é¡µå†…å®¹"
            }
        ]
    }


@router.get("/health")
async def chat_health_check():
    """
    Chat APIå¥åº·æ£€æŸ¥
    """
    return {
        "status": "healthy",
        "ai_service_available": azure_openai_service.is_available(),
        "supported_features": [
            "url_analysis",
            "content_extraction", 
            "ai_chat",
            "context_aware_chat"
        ]
    }